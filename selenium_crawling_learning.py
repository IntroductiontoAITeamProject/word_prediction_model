from gensim.models import Word2Vec
from konlpy.tag import Okt
import time
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium import webdriver
import json

with open("korean_frequency.json", encoding="utf-8") as f:
    data = json.load(f)
target_words = sorted({w for group in data.values() for w in group})


def fetch_sentences_selenium(word, max_pages=1):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # ì°½ ë„ìš°ì§€ ì•ŠìŒ
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(service=Service(
        ChromeDriverManager().install()), options=options)
    results = []

    for page in range(1, max_pages + 1):
        start = (page - 1) * 10 + 1
        url = f"https://search.naver.com/search.naver?where=news&query={word}&start={start}"
        print(f"ğŸ” [{word}] í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")

        try:
            driver.get(url)

            # í•´ë‹¹ í´ë˜ìŠ¤ì˜ í…ìŠ¤íŠ¸ê°€ ë‰´ìŠ¤ ì œëª©ìœ¼ë¡œ ë³´ì¸ë‹¤ë©´ ì´ ë°©ì‹ ì‚¬ìš©
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, ".sds-comps-text-type-headline1"))
            )
            articles = driver.find_elements(
                By.CSS_SELECTOR, ".sds-comps-text-type-headline1")
            for a in articles:
                text = a.text.strip()
                if text:
                    results.append(text)

        except Exception as e:
            print(f"âš ï¸ [{word}] í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")

    driver.quit()
    print(f"âœ… [{word}] ìˆ˜ì§‘ ì™„ë£Œ (ì´ {len(results)}ë¬¸ì¥)")
    return results


okt = Okt()


def preprocess_sentences(sentences):
    processed = []
    for s in sentences:
        words = okt.morphs(s, stem=True)
        if words:
            processed.append(words)
    return processed


all_sentences = []
for word in target_words[:100]:  # í…ŒìŠ¤íŠ¸ìš©: ìƒìœ„ 100ê°œ ë‹¨ì–´ë§Œ
    try:
        sents = fetch_sentences_selenium(word, max_pages=2)
        all_sentences.extend(sents)
    except Exception as e:
        print(f"[ì˜¤ë¥˜] {word}: {e}")

print(f"ì´ ìˆ˜ì§‘ëœ ë¬¸ì¥ ìˆ˜: {len(all_sentences)}")
print("ì˜ˆì‹œ ë¬¸ì¥:", all_sentences[0] if all_sentences else "ì—†ìŒ")


tokenized = preprocess_sentences(all_sentences)

model = Word2Vec(
    sentences=tokenized,
    vector_size=300,
    window=5,
    min_count=1,
    workers=4
)
model.wv.save("wiktionary5800_custom.kv")
print("âœ… Word2Vec ì €ì¥ ì™„ë£Œ")
